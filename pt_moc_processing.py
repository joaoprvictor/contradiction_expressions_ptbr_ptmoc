# -*- coding: utf-8 -*-
"""pt_moc_processing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xbSSDykttgljnJoTSxe9UpJHA0jVXLWr
"""

import os
# !pip install docx2txt
import re
# import docx2txt
import pandas as pd

# Specify the directory containing the .docx files.
docx_directory = '/content/drive/MyDrive/JVPESSOA D.&L CONSULTING/consultorias/br-mocam/port_mocambique/'
txt_directory = "/content/drive/MyDrive/JVPESSOA D.&L CONSULTING/consultorias/br-mocam/mocambique_txt/"

# Loop through all .docx files in the specified directory.
# conversion docx to txt
# for filename in os.listdir(docx_directory):
#     if filename.endswith('.docx'):
#         docx_path = os.path.join(docx_directory, filename)
#         text = docx2txt.process(docx_path)

#         # Create the corresponding .txt filename.
#         txt_filename = os.path.splitext(filename)[0] + '.txt'
#         txt_filename = re.sub(" - transcrição", "", txt_filename)
#         txt_path = os.path.join(txt_directory, txt_filename)

#         # Write the extracted text to the .txt file.
#         with open(txt_path, 'w', encoding='utf-8') as f:
#             f.write(text)

#         print(f"Conversion complete: {docx_path} -> {txt_path}")

# print("Batch conversion complete.")

# Initialize an empty list to store DataFrames for each file.
# dataframes = []

# Initialize empty lists to store file names and file contents
file_names = []
file_contents = []

# Loop through all .txt files in the specified directory.
for filename in os.listdir(txt_directory):
    if filename.endswith('.txt'):
        txt_path = os.path.join(txt_directory, filename)
        with open(txt_path, 'r') as file:
            lines = file.readlines()
            # Append the file name to the list
            new_file_name = re.sub("\.txt", "", filename)
            file_names.extend([new_file_name] * len(lines))
            # Append each line from the file to the list
            file_contents.extend(lines)

        # # Initialize lists to store speaker and text data.
        # speakers = []
        # texts = []

        # # Open and read the content of the .txt file.
        # with open(txt_path, 'r', encoding='utf-8') as f:
        #     lines = f.readlines()

        #     # Process each line in the file.
        #     for line in lines:
        #         line = line.strip()
        #         speaker = re.findall('^[A-Z]', line)
        #         speaker = " ".join(speaker)
        #         turn = re.sub("^[A-Z]: ", "", line)
        #     # Add the last speaker's turn.
        #         speakers.append(speaker)
        #         texts.append(turn)

        # # Create a DataFrame for this file.
        # df = pd.DataFrame({'Falante': speakers, 'Texto': texts})
        # dataframes.append(df)

# Create a DataFrame
data = {'Arquivo': file_names, 'Turno': file_contents}
df = pd.DataFrame(data)

def clean_df(df):
  df_clean = df[df["Arquivo"] != ""]
  df_clean = df_clean[df_clean["Arquivo"] != "\n"]
  df_clean = df_clean[df_clean["Turno"] != ""]
  df_clean = df_clean[df_clean["Turno"] != "\n"]
  return df_clean

df_cleaned1 = clean_df(df)

mask_entrevistador = df_cleaned1["Turno"].str.contains("^L:")

df_entrevistados = df_cleaned1[mask_entrevistador]

limpar = "\[|=|:|D: |((risos))|\.+|\/|\?|\n|L: |L |\.{3}"
df_entrevistados['Turno'] = df_entrevistados['Turno'].apply(lambda x: re.sub(limpar, "", x))
df_entrevistados_clean = clean_df(df_entrevistados)
df_entrevistados_clean['Turno'] = df_entrevistados_clean['Turno'].apply(lambda x: re.sub("\s+", " ", x))
df_entrevistados_clean['Turno'] = df_entrevistados_clean['Turno'].apply(lambda x: re.sub("^\s+", "", x))

# Define a function to count tokens in a text
def count_tokens(text):
    tokens = text.split()
    return len(tokens)


df_entrevistados_clean['num_tokens'] = df_entrevistados_clean["Turno"].apply(count_tokens)

expressions = ['antes','ao passo que','contudo',' e sim','em todo caso','entretanto',
               'mas ', 'não obstante','no entanto','porém','senão','só que ','todavia']

# Create a regular expression pattern by joining your 'expressions' list with '|'
pattern = '|'.join(expressions)

# Filter the DataFrame based on the 'Texto' column using the regular expression pattern
filtered_df = df_entrevistados_clean[df_entrevistados_clean['Turno'].str.contains(pattern, case=False, regex=True)]

# Add a new column 'Expressão' that contains the matched expressions for each row
filtered_df['Expressão'] = filtered_df['Turno'].str.findall(pattern)

filtered_df[:5]

filtered_df_exploded = filtered_df.explode('Expressão')
# Get a list of unique strings in the "Expressão" column
unique_strings = filtered_df_exploded['Expressão'].unique()

# Create new columns for each unique string and count their occurrences in each row
for string in unique_strings:
    filtered_df_exploded[string] = filtered_df_exploded['Expressão'].apply(lambda x: str(x).count(str(string)))

# # Group by the original index (assuming you want to keep the original row structure)
new_df_filtered = filtered_df_exploded.groupby(filtered_df_exploded.index).max()

# Reset the index if needed
new_df_filtered = new_df_filtered.reset_index()

new_df_filtered = new_df_filtered[['Arquivo','Turno','num_tokens','Expressão',
                                   'antes','só que ','mas ','porém','senão',
                                   ' e sim','entretanto', 'ao passo que',   'no entanto']]

# dataframe com as informações de contexto
path = '/content/drive/MyDrive/JVPESSOA D.&L CONSULTING/consultorias/br-mocam/resultados/'
new_df_filtered.to_csv(path+"pt_moc_turnos.csv")
new_df_filtered.to_excel(path+"pt_moc_turnos.xlsx")

grouped_df = new_df_filtered.groupby('Arquivo')[['num_tokens',
                                   'antes','só que ','mas ','porém','senão',
                                   ' e sim','entretanto', 'ao passo que',   'no entanto']].sum()

# Reset the index if needed
grouped_df = grouped_df.reset_index()

grouped_df["Idade"] = grouped_df["Arquivo"].apply(lambda x: "".join(re.findall("PMO([A-Z])", x)))
grouped_df["Escolaride"] = grouped_df["Arquivo"].apply(lambda x: "".join(re.findall("PMO[A-Z]([0-9])", x)))
grouped_df["Gênero"] = grouped_df["Arquivo"].apply(lambda x: "".join(re.findall("PMO[A-Z][0-9]([A-Z])", x)))

# Define the columns to perform the operation on
columns_to_transform = ['antes','só que ','mas ','porém','senão',
                                   ' e sim','entretanto', 'ao passo que',   'no entanto']

# Create new columns with the normalized values
for col in columns_to_transform:
    new_col_name = col + '_normalizado'
    grouped_df[new_col_name] = round((grouped_df[col] / grouped_df['num_tokens'].sum()) * 1000, 2)

# Define a function to count the total number of string items in a list
def count_string_items(lst):
    return len(lst)

# INFORMAÇÕES SOBRE O DATASET
info1 = f"o número final de falantes é {len(grouped_df)}\n====\n"
info2 = f"o número total de tokens (contagem bruta) é {grouped_df['num_tokens'].sum()}\n====\n"
print(info1, info2)

with open("informações_ptmoc.txt", "a") as corpus_info_txt:
  corpus_info_txt.write(info1)
  corpus_info_txt.write(info2)

column_name_mapping = {
    'só que ': 'só que',
    'mas ': 'mas',
    ' e sim': 'e sim',
    'só que _normalizado': 'só que_normalizado',
    'mas _normalizado': 'mas_normalizado',
    ' e sim_normalizado': 'e sim_normalizado'
}
# Use the rename method to rename the columns
grouped_df.rename(columns=column_name_mapping, inplace=True)

grouped_df.columns

grouped_new_order = grouped_df[['Arquivo', 'Idade',
       'Escolaride', 'Gênero','num_tokens', 'antes', 'só que', 'mas', 'porém', 'senão',
       'e sim', 'entretanto', 'ao passo que', 'no entanto',  'antes_normalizado', 'só que_normalizado',
       'mas_normalizado', 'porém_normalizado', 'senão_normalizado',
       'e sim_normalizado', 'entretanto_normalizado',
       'ao passo que_normalizado', 'no entanto_normalizado'
                                ]]

grouped_new_order[:5]

# dataframe com as informações de contagem
path = '/content/drive/MyDrive/JVPESSOA D.&L CONSULTING/consultorias/br-mocam/resultados/'
grouped_new_order.to_csv(path+"pt_moc_contagem.csv")
grouped_new_order.to_excel(path+"pt_moc_contagem.xlsx")

grouped_new_order.columns

# Select only the numeric columns
numeric_columns = ['antes',
       'só que', 'mas', 'porém', 'senão', 'e sim', 'entretanto',
       'ao passo que', 'no entanto']
# Calculate the total sum of specific numeric columns for each row and store it in a new column
grouped_new_order['soma_adversativas'] = grouped_new_order[numeric_columns].sum(axis=1)

# Select only the numeric columns part 2
numeric_columns_normalized = ['antes_normalizado', 'só que_normalizado',
       'mas_normalizado', 'porém_normalizado', 'senão_normalizado',
       'e sim_normalizado', 'entretanto_normalizado',
       'ao passo que_normalizado', 'no entanto_normalizado']
# Calculate the total sum of specific numeric columns for each row and store it in a new column
grouped_new_order['soma_adversativas_normalizada'] = grouped_new_order[numeric_columns_normalized].sum(axis=1)

info3 = f'O número de pessoas por grau de escolaridade é \n{grouped_new_order["Escolaride"].value_counts()}\n============\n'
info4 = f'O número de pessoas por faixa etária é \n{grouped_new_order["Idade"].value_counts()}\n============\n'
info5 = f'O número de pessoas por gênero é \n{grouped_new_order["Gênero"].value_counts()}\n============\n'
with open("informações_ptmoc.txt", "a") as corpus_info_txt:
  corpus_info_txt.write(info3)
  corpus_info_txt.write(info4)
  corpus_info_txt.write(info5)

info6 = f'O número de tokens por grau de escolaridade é \n{grouped_new_order.groupby("Idade")["num_tokens"].sum()}\n============\n'
info7 = f'O número de tokens por faixa etária é \n{grouped_new_order.groupby("Escolaride")["num_tokens"].sum()}\n============\n'
info8 = f'O número de tokens por gênero é \n{grouped_new_order.groupby("Gênero")["num_tokens"].sum()}\n============\n'
with open("informações_ptmoc.txt", "a") as corpus_info_txt:
  corpus_info_txt.write(info6)
  corpus_info_txt.write(info7)
  corpus_info_txt.write(info8)

info9 = f'O número de expressões adversativas (contagem bruta) por grau de escolaridade é \n{grouped_new_order.groupby("Idade")["soma_adversativas"].sum()}\n============\n'
info10= f'O número de expressões adversativas (contagem bruta)por faixa etária é \n{grouped_new_order.groupby("Escolaride")["soma_adversativas"].sum()}\n============\n'
info11 = f'O número de expressões adversativas (contagem bruta) por gênero é \n{grouped_new_order.groupby("Gênero")["soma_adversativas"].sum()}\n============\n'
info12 = f'O número TOTAL de expressões adversativas (contagem bruta) é \n{grouped_new_order["soma_adversativas"].sum()}\n============\n'
with open("informações_ptmoc.txt", "a") as corpus_info_txt:
  corpus_info_txt.write(info12)
  corpus_info_txt.write(info9)
  corpus_info_txt.write(info10)
  corpus_info_txt.write(info11)

import matplotlib.pyplot as plt

# Flatten the list of lists in the "Expressão" column into a single list of strings
expressions = [expr for sublist in filtered_df['Expressão'] for expr in sublist]
# Create a pandas Series from the flattened list
expressions_series = pd.Series(expressions)

# Use value_counts() to count the occurrences of each string
expression_counts = expressions_series.value_counts()

# Plot the counts
expression_counts.plot(kind='bar', figsize=(10, 6))
plt.xlabel('Expressões de adversidade')
plt.ylabel('Quantidade')
plt.title('Número de expressões adversativas no Corpus PT-MOC')
plt.savefig('/content/drive/MyDrive/JVPESSOA D.&L CONSULTING/consultorias/br-mocam/resultados/n_expressoes_ptmoc.png',
            dpi=300, bbox_inches="tight")
plt.show()